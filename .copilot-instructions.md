# Copilot Instructions for Project Marcel Backend

## System Overview

This is **Project Marcel**, a sophisticated NestJS-based web scraping and content management system for the Swiss transportation industry. The system automatically scrapes content from 35+ railway and transportation websites, enhances it with AI, and provides a comprehensive REST API.

## Key Architecture Patterns

### 1. Modular NestJS Structure
```
src/modules/
├── scraper/           # Web scraping (main feature)
├── articles/          # Content management
└── content-generator/ # AI enhancement
```

### 2. Two-Stage Scraping Pattern
Every website scraper follows this pattern:
- **Stage 1**: List scraping (extract metadata from listing pages)
- **Stage 2**: Deep scraping (extract full content from individual articles)

### 3. Service Layer Architecture
- `ScraperService`: Orchestrates scraping workflows
- `ScraperDeeperService`: Handles deep content extraction
- `ArticlesService`: Database operations with MongoDB

## Content Types & Data Model

### Article Types (Enum)
- `Video`: YouTube videos from transportation channels
- `News`: News articles from railway/transport websites
- `LinkedIn`: Company posts from transportation LinkedIn pages

### Article Schema (MongoDB)
```typescript
{
  baseUrl: string;           // Source website
  url: string;              // Unique identifier (except LinkedIn)
  title: string;            // Article title
  type: ArticleType;        // Video | News | LinkedIn
  dateText: string;         // Original date string
  date: Date;              // Parsed date object
  image: string;           // Original image URL
  imageLocal?: string;     // Local image path
  originalContent: string; // Scraped content
  generatedContent?: string; // AI-enhanced content
  metadata?: object;       // Additional data (views, duration, etc.)
}
```

## Scraping Scripts Location & Naming

All website scrapers are in: `/src/modules/scraper/services/scraping-scripts/`

### Naming Convention
- File: `{website-name}.script.ts`
- Functions: `getAll{WebsiteName}Articles()` and `get{WebsiteName}Article(url)`

### Existing Scrapers (35+ websites)
Major sources include:
- **Railway**: `sbb.script.ts`, `bls.script.ts`, `rhb.script.ts`, `sob.script.ts`
- **Public Transport**: `bernmobil.script.ts`, `zvv.script.ts`, `vvl.script.ts`
- **Industry**: `alstom.script.ts`, `abb.script.ts`, `doppelmayr.script.ts`
- **News**: `lok-report.script.ts`, `railmarket.script.ts`, `baublatt.script.ts`
- **Special**: `youtube.script.ts`, `linkedIn.script.ts`

## Code Patterns & Conventions

### 1. Scraper Script Template
```typescript
import { getPuppeteerInstance } from 'src/common/utils/puppeteer-instance';
import { ArticleType } from 'src/models/articles.models';

// List scraping
export const getAllWebsiteArticles = async () => {
  const { browser, page } = await getPuppeteerInstance();
  
  try {
    await page.goto('website-url', { waitUntil: 'networkidle2' });
    
    const articles = await page.evaluate((articleType) => {
      return Array.from(document.querySelectorAll('.selector')).map((article) => ({
        baseUrl: window.location.href,
        type: articleType,
        title: article.querySelector('h3')?.innerText?.trim() || 'N/A',
        url: article.querySelector('a')?.href || '',
        dateText: article.querySelector('.date')?.innerText?.trim() || 'N/A',
        image: article.querySelector('img')?.src || 'N/A',
        teaser: article.querySelector('.summary')?.innerText?.trim() || 'N/A'
      }));
    }, ArticleType.News);
    
    return articles;
  } finally {
    await browser.close();
  }
};

// Deep content scraping
export const getWebsiteArticle = async (pageUrl: string) => {
  const { browser, page } = await getPuppeteerInstance();
  
  try {
    await page.goto(pageUrl, { waitUntil: 'networkidle2' });
    
    const content = await page.evaluate(() => {
      return document.querySelector('.article-content')?.innerText || '';
    });
    
    return content;
  } finally {
    await browser.close();
  }
};
```

### 2. Service Integration Pattern
When adding new scrapers, update both services:

**ScraperService** (`scraper.service.ts`):
```typescript
async getAllWebsiteArticles() {
  const articles = await getAllWebsiteArticles();
  
  for (let article of articles) {
    article['originalContent'] = await this.scraperDeeperService.getWebsiteArticle(article.url);
    await this.articlesService.createArticle(article);
  }
  
  return articles;
}
```

**ScraperDeeperService** (`scraper-deeper.service.ts`):
```typescript
async getWebsiteArticle(pageUrl: string) {
  return getWebsiteArticle(pageUrl);
}
```

### 3. Error Handling Standards
```typescript
try {
  await page.goto(url, { waitUntil: 'networkidle2', timeout: 30000 });
} catch (error) {
  console.error(`Failed to load ${url}:`, error);
  return []; // Return empty array, don't throw
}
```

### 4. Deduplication Logic
- **News/Video**: Uses `url` field as unique identifier
- **LinkedIn**: Content-based deduplication (same content = duplicate)

## Key Utilities

### 1. Puppeteer Instance
```typescript
// Always use this utility for consistent browser setup
import { getPuppeteerInstance } from 'src/common/utils/puppeteer-instance';

const { browser, page } = await getPuppeteerInstance();
// Always close browser in finally block
```

### 2. Date Formatting
```typescript
import { formatDate, parseRelativeDate, parseRelativeDateLinkedIn } from 'src/common/utils/format-date';

// Handles various European date formats
const standardDate = formatDate('28.01.2025'); // -> '28/01/2025'

// Handles relative dates
const absoluteDate = parseRelativeDate('2 days ago'); // -> Date object
```

### 3. Image Processing
```typescript
import { enhanceImage } from 'src/common/utils/enhance-image';
import { downloadImage } from 'src/common/helpers/download-image';

// Download and enhance images
const filename = await downloadImage(imageUrl);
const enhanced = await enhanceImage(filename);
```

## API Endpoints Structure

### Scraper Endpoints (`/scraper`)
- `GET /scraper/test` - Database connection test
- `GET /scraper/{channelName}` - YouTube channel scraping
- `GET /scraper/{channelName}/{term}` - YouTube search
- `GET /scraper/linkedin/{company}` - LinkedIn scraping
- `GET /scraper/download` - Image processing
- `GET /scraper/formateDates` - Date standardization

### Articles Endpoints (`/articles`)
- `GET /articles` - Paginated article list
- `GET /articles/videos` - Video articles only
- `GET /articles/linkedin` - LinkedIn posts only
- `GET /articles/search?q=term` - Text search
- `GET /articles/vrandom` - Random videos
- `GET /articles/nrandom` - Random news

### Content Generator (`/content-generator`)
- `GET /content-generator/content` - AI content generation
- `GET /content-generator/teaser` - AI teaser creation
- `GET /content-generator/video` - Video summaries
- `GET /content-generator/image-title` - Image context

## Database Operations

### ArticlesService Key Methods
```typescript
// Create with automatic deduplication
await this.articlesService.createArticle(article);

// Query methods
this.articlesService.findAll()              // All articles
this.articlesService.findNoContent()        // Missing AI content
this.articlesService.findNoTeaser()         // Missing teasers
this.articlesService.findVideoNoSummary()   // Videos without summaries

// Update methods
this.articlesService.updateContent(id, content)
this.articlesService.updateTeaser(id, teaser)
this.articlesService.updateImageTitleContext(id, context)
```

## Environment & Configuration

### Key Environment Variables
```bash
NODE_ENV=development|production
MONGO_URI=mongodb://localhost:27017/practicedb
OPENAI_API_KEY=sk-...                # For AI content generation
PORT=3000
```

### MongoDB Connection
- Database: `practicedb`
- Collection: `articles`
- Auto-connects via Mongoose in `app.module.ts`

## Development Guidelines

### 1. Adding New Website Scrapers
1. Create script file: `/scraping-scripts/{website}.script.ts`
2. Implement `getAll{Website}Articles()` and `get{Website}Article(url)`
3. Add methods to both `ScraperService` and `ScraperDeeperService`
4. Test with actual website
5. Handle errors gracefully

### 2. Testing New Scrapers
```bash
# Start development server
npm run start:dev

# Test individual scraper via API
curl http://localhost:3000/scraper/test

# Check logs in console and log files
tail -f log/debug/debug.log
```

### 3. Common Scraping Challenges
- **Cookie Banners**: Handle with try-catch blocks
- **Dynamic Content**: Use `waitForSelector()` or scrolling
- **Rate Limiting**: Add delays between requests
- **Anti-bot Measures**: Use stealth mode, vary user agents

### 4. Code Quality
- Always close Puppeteer browsers in `finally` blocks
- Use TypeScript types consistently
- Follow existing error handling patterns
- Add appropriate logging for debugging

## AI Content Generation

The system uses OpenAI GPT to enhance scraped content:
- **Content Generation**: Transform raw scrapes into structured articles
- **Teaser Creation**: Generate compelling summaries
- **Image Context**: Create descriptive alt-text
- **Better Images**: Find higher quality replacements

Integration happens via `ContentGeneratorService` with separate endpoints.

## Logging & Debugging

### Winston Logging
```typescript
// Logs automatically go to:
log/debug/debug.log    // All levels
log/error/error.log    // Errors only  
log/info/info.log      // Info and above
```

### Debug Mode
```bash
# Enable Puppeteer debugging
DEBUG=puppeteer:* npm run start:dev

# Show browsers during scraping (set in puppeteer-instance.ts)
headless: false
```

## Performance Considerations

### 1. Memory Management
- Always close Puppeteer browsers
- Process articles in batches for large sites
- Implement garbage collection hints for large operations

### 2. Database Performance
- URLs are unique indexes for deduplication
- Consider pagination for large result sets
- Use appropriate MongoDB indexes

### 3. Scraping Etiquette
- Add delays between requests
- Respect robots.txt when possible
- Monitor for rate limiting responses

## Common Patterns & Anti-Patterns

### ✅ Good Patterns
```typescript
// Proper error handling
try {
  const articles = await getAllWebsiteArticles();
  return articles;
} catch (error) {
  console.error('Scraping failed:', error);
  return []; // Don't crash the entire process
}

// Consistent data structure
const article = {
  baseUrl: window.location.href,
  type: ArticleType.News,
  title: title?.trim() || 'N/A',
  url: url || '',
  // ... other fields
};

// Always clean up resources
try {
  // scraping logic
} finally {
  await browser.close();
}
```

### ❌ Anti-Patterns
```typescript
// Don't throw errors for missing elements
const title = article.querySelector('h2').innerText; // ❌ Can crash

// Don't hardcode values
url: 'https://example.com' + linkElement.href; // ❌ Fragile

// Don't forget cleanup
const browser = await puppeteer.launch();
// ... scraping logic
// Missing: await browser.close(); // ❌ Memory leak
```

## Integration with Frontend

The frontend (Next.js) consumes this API through:
- React Query hooks (`useApi.ts`)
- Service layer (`ArticlesService`, `ScraperService`, `ContentGeneratorService`)
- TypeScript interfaces for type safety

When adding new endpoints, ensure corresponding frontend hooks are created.

## Security Considerations

### 1. LinkedIn Scraping
- Credentials are hardcoded in `linkedIn.script.ts`
- Consider using environment variables for production
- Respect LinkedIn's terms of service

### 2. Input Validation
- All DTOs use `class-validator`
- API endpoints validate input parameters
- Database operations use Mongoose schema validation

### 3. Rate Limiting
- Consider implementing throttling for scraping endpoints
- Monitor for abuse patterns
- Add authentication for sensitive operations

## Deployment Notes

### Development
```bash
npm run start:dev  # Hot reload enabled
```

### Production
```bash
npm run build
npm run start:prod
```

### Docker
- Puppeteer requires additional setup in containers
- Chrome/Chromium must be installed in image
- Consider using official Puppeteer Docker images

## Troubleshooting Guide

### Common Issues
1. **"MongoDB connection failed"** → Check if MongoDB is running
2. **"Browser launch failed"** → Install Chrome/Chromium
3. **"OpenAI API error"** → Verify API key and quota
4. **"Scraping timeout"** → Check website accessibility and selectors
5. **"Memory issues"** → Ensure browsers are properly closed

### Debug Steps
1. Check environment variables
2. Verify MongoDB connection
3. Test individual scraper scripts
4. Check browser console for JavaScript errors
5. Review log files for detailed error information

---

This copilot instructions file provides comprehensive context about the Project Marcel backend system architecture, patterns, and conventions. Use this information to maintain consistency when adding new features, fixing bugs, or extending the scraping capabilities.
